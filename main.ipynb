{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLe7SJzkOs5mKEUKNlqVXz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Manual Labor\n",
        "Manually created Gen Z phrases (ofc generated by ChatGPT because I don't feel like sorting through all of this myself and defining whether it's positive or negative).\n",
        "\n",
        "> **Note**: This isn't returned back the user, it's used to train the neural net to generate authentic sentiment responses."
      ],
      "metadata": {
        "id": "Y-FZmi0FSryi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually created list of Gen Z phrases and their sentiment labels\n",
        "gen_z_phrases = [    (\"boujee af\", 1),    (\"on fleek\", 1),    (\"goals\", 1),    (\"on point\", 1),    (\"fire\", 1),    (\"yasss\", 1),    (\"lit fam\", 1),    (\"bae\", 1),    (\"slay\", 1),    (\"queen\", 1),    (\"king\", 1),    (\"go hard\", 1),    (\"boss\", 1),    (\"badass\", 1),    (\"epic\", 1),    (\"legendary\", 1),    (\"fierce\", 1),    (\"dope\", 1),    (\"sick\", 1),    (\"crushin' it\", 1),    (\"stunner\", 1),    (\"power move\", 1),    (\"jaw-dropping\", 1),    (\"mind-blowing\", 1),    (\"out of this world\", 1),    (\"smooth operator\", 1),    (\"game-changer\", 1),    (\"genius\", 1),    (\"prodigy\", 1),    (\"phenomenal\", 1),    (\"majestic\", 1),    (\"divine\", 1),    (\"goddess\", 1),    (\"godlike\", 1),    (\"trendsetter\", 1),    (\"innovator\", 1),    (\"disruptor\", 1),    (\"influencer\", 1),    (\"pioneer\", 1),    (\"trailblazer\", 1),    (\"frontrunner\", 1),    (\"visionary\", 1),    (\"icon\", 1),    (\"superstar\", 1),    (\"guru\", 1),    (\"maestro\", 1),    (\"expert\", 1),    (\"ace\", 1),    (\"champion\", 1),    (\"winner\", 1),    (\"trash af\", 0),    (\"wack af\", 0),    (\"dead af\", 0),    (\"lame\", 0),    (\"boring\", 0),    (\"weak\", 0),    (\"lousy\", 0),    (\"useless\", 0),    (\"inferior\", 0),    (\"disappointing\", 0),    (\"lacking\", 0),    (\"mediocre\", 0),    (\"subpar\", 0),    (\"second-rate\", 0),    (\"unsatisfactory\", 0),]"
      ],
      "metadata": {
        "id": "eY-9NifoUujv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How does it work\n",
        "\n",
        "The program performs sentiment analysis on a set of Gen Z phrases using TensorFlow. The input is a list of Gen Z phrases along with their sentiment labels (positive or negative). The list is then converted into arrays of phrases and labels.\n",
        "\n",
        "The phrases are then tokenized, meaning that each phrase is converted into a sequence of numbers where each number represents a word in the phrase. The sequences are then padded to ensure that all sequences have the same length.\n",
        "\n",
        "The model architecture is defined using TensorFlow's Sequential API. The model consists of an Embedding layer, GlobalAveragePooling1D layer, 2 Dense layers with relu activation function, and a final Dense layer with a sigmoid activation function. The model is then compiled with a binary_crossentropy loss function and an Adam optimizer.\n",
        "\n",
        "The model is then trained using the padded sequences and labels for 5 epochs. The model's performance is evaluated on the training data and the results are printed.\n",
        "\n",
        "Finally, the program has a function `predict_sentiment` that takes a Gen Z slang phrase as input, tokenizes and pads the phrase, and returns the sentiment prediction of the model. The sentiment prediction is a value between 0 and 1, where values greater than `0.5` indicate a positive response and values less than `0.5` indicate a negative response."
      ],
      "metadata": {
        "id": "qSJcKcHCVqXw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEWuVloDR1j4",
        "outputId": "c47bec2a-a84d-4321-e184-7f3cd07e8269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "3/3 [==============================] - 1s 4ms/step - loss: 0.6933 - accuracy: 0.3846\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.8000\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6872 - accuracy: 0.7846\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6848 - accuracy: 0.7692\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6822 - accuracy: 0.7692\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6797 - accuracy: 0.7692\n",
            "Test loss: 0.6797, accuracy: 0.7692\n",
            "Enter a phrase to predict\n",
            "> big yikes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe19eccab80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 55ms/step\n",
            "Negative statement: 0.49795085\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Convert the list of phrases and labels to arrays\n",
        "phrases, labels = zip(*gen_z_phrases)\n",
        "labels = np.array(labels, dtype=np.float32)\n",
        "\n",
        "# Tokenize the phrases and convert to arrays\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(phrases)\n",
        "sequences = tokenizer.texts_to_sequences(phrases)\n",
        "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding=\"post\")\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=100, output_dim=32),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(padded, labels, epochs=5)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "results = model.evaluate(padded, labels)\n",
        "print(\"Test loss: {:.4f}, accuracy: {:.4f}\".format(*results))\n",
        "\n",
        "def predict_sentiment(phrase):\n",
        "  sequences = tokenizer.texts_to_sequences([phrase])\n",
        "  padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding=\"post\")\n",
        "  prediction = model.predict(padded)\n",
        "  return prediction[0][0]\n",
        "\n",
        "phrase = input(\"Enter a phrase to predict\\n> \")\n",
        "sentiment = predict_sentiment(phrase)\n",
        "if sentiment > 0.5:\n",
        "  print(\"Positive statement: \" + str(sentiment))\n",
        "else:\n",
        "  print(\"Negative statement: \" + str(sentiment))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v3az7qHCTSlw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}